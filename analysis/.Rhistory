Function to load article text and calculate frequency scores:
```{r}
getTextFromParticipantFile = function(partNum,ids){
print(partNum)
fn = paste0("../data/HumanJudgements/stimuli/NewsArticles_",
partNum,"_SENT.txt")
tx = readLines(fn)
tx = paste(tx,collapse="\n")
tx = gsub("\n  [0-9][0-9]?\n","\n\n",tx)
tx = strsplit(tx,"\n [0-9][0-9]? : COP")[[1]]
idx = str_extract(tx,"[0-9][0-9]?_UK[0-9]+")
idx = paste0("COP",idx)
tx[match(ids,idx)]
}
processFile = function(d, accessibilityKW,securityKW,sustainabilityKW,
refFreqAcc,refFreqSec,refFreqSus){
# Get text from file sent to participant
d$text = ""
for(px in unique(d$participant)){
d[d$participant==px,]$text =
getTextFromParticipantFile(px,d[d$participant==px,]$ID)
}
# Lower case
d$text = tolower(d$text)
# some texts need to be borrowed from other files
d[is.na(d$text),]$text = d[match(d[is.na(d$text),]$ID, d$ID),]$text
# Create corpus, tokens, freq matrix
corp = corpus(d, docid_field = "ID2",text_field = "text")
tok = tokens(corp, remove_punct = TRUE)
corpDFM = dfm(tok)
d$ArticleTotalWords = rowSums(corpDFM)
# Get frequency for one keyword
getFrequency = function(keyword){
keyword = tolower(keyword)
if(grepl(" ",keyword)){
# Multi-word expression
return(sapply(str_extract_all(d$text, keyword),length))
}
if(keyword %in% colnames(corpDFM)){
return(as.vector(corpDFM[,keyword]))
}
return(rep(0,nrow(d)))
}
# Get score (frequency per 1000 words)
getScore = function(keywords){
freq = sapply(keywords,getFrequency)
return(rowSums(freq))
#prop = 1000000 * (freq/totalWords)
#return(prop)
}
d$CorpAccFreq = getScore(accessibilityKW)
d$CorpSecFreq = getScore(securityKW)
d$CorpSusFreq = getScore(sustainabilityKW)
d$CorpAccFreqRel = (1000000 * d$CorpAccFreq) / d$ArticleTotalWords
d$CorpSecFreqRel = (1000000 * d$CorpSecFreq) / d$ArticleTotalWords
d$CorpSusFreqRel = (1000000 * d$CorpSusFreq) / d$ArticleTotalWords
getScoreG2 = function(keywords,kFreq){
freq = sapply(keywords,getFrequency)
refFreq = kFreq[match(colnames(freq),kFreq$Item),]$Frequency
refFreq[is.na(refFreq)] = 0
G2s = sapply(1:nrow(freq),function(i){
logLikelihood.G2(freq[i,],refFreq,
d$ArticleTotalWords[i],482360)
})
return(colMeans(G2s,na.rm = T))
}
d$CorpSusFreqG2 = getScoreG2(sustainabilityKW,refFreqSus)
return(d)
}
```
```{r}
d4 = processFile(d4,
accessibilityKeywords, securityKeywords, sustainabilityKeywords,
refFreqAcc,refFreqSec,refFreqSus)
d4$Sustainability.scaled = d4$Sustainability/10
d4$CorpSusFreq.scaled = d4$CorpSusFreq/max(d4$CorpSusFreq)
```
\clearpage
\newpage
# Results
## Agreement between human participants
Estimate inter-rater reliability using Intraclass Correlation Coefficient:
```{r}
commonIDs = table(d4$ID)
commonIDs = names(commonIDs)[commonIDs>6]
irrx = d4[d4$ID %in% commonIDs,]
commonIDs
table(d4$ID)
sort(table(d4$ID))
commonIDs = table(d4$ID)
commonIDs = names(commonIDs)[commonIDs>6]
irrx = d4[d4$ID %in% commonIDs,]
irrx = irrx[order(irrx$participant,irrx$ID),]
irrx = matrix(irrx$Sustainability,
ncol=length(unique(irrx$participant)))
icc(irrx, model = "oneway")
irrx
d4[d4$participant==11,]$Sustainability
d4[d4$participant==11 & d4$ID %in% commonIDs,]$Sustainability
commonIDs = table(d4$ID)
commonIDs = names(commonIDs)[commonIDs>6]
irrx = d4[d4$ID %in% commonIDs,]
irrx = d4[d4$ID %in% commonIDs,]
table(irrx$participant,irrx$ID2)
table(irrx$participant,irrx$ID)
table(irrx$ID, irrx$participant)
commonIDs = table(d4$ID)
commonIDs = names(commonIDs)[commonIDs>6]
irrx = d4[d4$ID %in% commonIDs,]
irrx = irrx[!duplicated(paste(d4$participant,d4$ID)),]
irrx = irrx[order(irrx$participant,irrx$ID),]
irrx = matrix(irrx$Sustainability,
ncol=length(unique(irrx$participant)))
commonIDs = table(d4$ID)
commonIDs = names(commonIDs)[commonIDs>6]
irrx = d4[d4$ID %in% commonIDs,]
irrx = irrx[!duplicated(paste(d4$participant,d4$ID)),]
table(irrx$ID, irrx$participant)
sum(duplicated(paste(d4$participant,d4$ID)))
irrx = irrx[!duplicated(paste(d4$participant,d4$ID)),]
table(irrx$ID, irrx$participant)
commonIDs = table(d4$ID)
commonIDs = names(commonIDs)[commonIDs>6]
irrx = d4[d4$ID %in% commonIDs,]
irrx = irrx[!duplicated(paste(irrx$participant,irrx$ID)),]
table(irrx$ID, irrx$participant)
irrx = matrix(irrx$Sustainability,
ncol=length(unique(irrx$participant)))
commonIDs = table(d4$ID)
commonIDs = names(commonIDs)[commonIDs>6]
irrx = d4[d4$ID %in% commonIDs,]
irrx = irrx[!duplicated(paste(irrx$participant,irrx$ID)),]
irrx = irrx[order(irrx$participant,irrx$ID),]
irrx = matrix(irrx$Sustainability,
ncol=length(unique(irrx$participant)))
icc(irrx, model = "oneway")
parts = as.numeric(sort(unique(d4$participant)))
parts = parts[!parts %in% c(11)]
cors = matrix(NA,nrow=length(parts),ncol=length(parts))
rownames(cors) = parts
colnames(cors) = parts
d4 = d4[order(d4$ID),]
for(i in 1:length(parts)){
for(j in 1:length(parts)){
part1 = parts[i]
part2 = parts[j]
p1 = d4[d4$participant==part1 & d4$ID %in% commonIDs,]
p2 = d4[d4$participant==part2 & d4$ID %in% commonIDs,]
cors[i,j] = cor(p1$Sustainability,p2$Sustainability,
method = "kendall")
}
}
diag(cors)=NA
mean(cors[lower.tri(cors)],na.rm=T)
hist(cors)
mean(cors[-1,-1],na.rm=T)
cors
mean(cors[-1,-1],na.rm=T)
meanCorBetweenHumans = mean(cors[-1,-1],na.rm=T)
cx = cors[-1,-1]
cx = cx[lower.tri(cx)]
sdCorBetweenHumans = sd(cx)
meanCorBetweenHumans
meanCorBetweenHumans
sdCorBetweenHumans
icc(irrx[,-1], model = "oneway")
colMeans(cors)
colMeans(cors,na.rm = T)
mean(cors[c(-1,-8),c(-1,-8)],na.rm=T)
d4 = d4[d4$participant!=1,]
d4$participant = factor(d4$participant)
mean(d4$Sustainability)
mean(d4$Security)
mean(d4$Accessibility)
heatmap(cors)
?heatmap(cors,)
?heatmap(cors,Rowv=NA,Colv="Rowv")
heatmap(cors,Rowv=NA,Colv="Rowv")
cors
parts = as.numeric(sort(unique(d4$participant)))
parts = parts[!parts %in% c(11)]
cors = matrix(NA,nrow=length(parts),ncol=length(parts))
rownames(cors) = parts
colnames(cors) = parts
d4 = d4[order(d4$ID),]
for(i in 1:length(parts)){
for(j in 1:length(parts)){
part1 = parts[i]
part2 = parts[j]
p1 = d4[d4$participant==part1 & d4$ID %in% commonIDs,]
p2 = d4[d4$participant==part2 & d4$ID %in% commonIDs,]
cors[i,j] = cor(p1$Sustainability,p2$Sustainability,
method = "kendall")
}
}
---
title: "Human judgements of energy trilemma discussion"
output:
pdf_document:
toc: true
editor_options:
chunk_output_type: console
---
```{r echo=F,eval=F}
try(setwd("~/OneDrive - Cardiff University/Research/Cardiff/ClimageChangeAndLanguage/project/analysis/"))
```
# Introduction
To validate the automated measure, we compared it to human judgements. 10 fluent English speakers in the UK were trained on the basics of the energy trilemma, using standard teaching resources on the topic (Glasgow Science Centre, 2021; Our Future Energy, 2022, see below). Participants were not told what the key terms were.
Each participant was asked to read 40 randomly selected articles from the corpus. For each article, they rated the extent to which each article discussed each of the three aspects of the energy trilemma, scoring each aspect independently from 0 to 10. One participant was excluded due to technical difficulties (see discussion below). 8 of the articles were identical across participants, and these were used to test the agreement between human rates.
The three measures are not independent, since discussing one aspect of the trilemma usually means not discussing the others. Therefore, the analyses below focus on sustainability, since that is the best-represented topic.
The instructions for participants are included below:
\clearpage
\newpage
# Instructions for participants
This project aims to measure how much the media discusses the ‘energy trilemma’: three crucial components of our global energy system. These are the topics of accessibility, security, and sustainability.
We’ve collected a large number of news articles about the UN conferences on climate change (e.g. COP26). We’re using automatic computational linguistics methods to summarise how much discussion is occurring for each of the three areas. However, we’re not sure whether the computational methods align well with human judgements. That’s where you come in.
You’ll read a short news article, then give your opinion about how much each part of the trilemma was discussed. We’ll then use this data to check that the computational method is behaving sensibly.
The first task is for you to understand what the three aspects of the trilemma are, so that you can recognise them. There’s a short introduction to the topics on the next page, and a link to a short video.
Rating involves the following steps:
-  Open the “NewsArticles” pdf file and read one of the articles. Some of the formatting might be a little strange or the text might be cut off halfway through a sentence. Don’t worry about this, just try to get a gist of what the article is about.
-  When you’ve read the article, open up the ratings excel file. You can provide your judgement about how much the article discussed each component. Make sure the article number and article ID match.
-  Type in a score from 0 (did not discuss) to 10 (discussed a lot) for each article. Some articles might not discuss one of the components at all, while in others there may be a balance. Looking at many articles together, we’re guessing that there will be a general balance, but we could be wrong. Don’t overthink things – we want to know your overall impression of each article.
Go through each of the 40 articles in the article list and rate each one. It should take about 4 hours. You’re not expected to do this in one sitting.
## The Energy Trilemma
We use energy to power our phones and TVs, to heat our houses, cook our food, and transport us by car, train and plane. Sources of energy include oil, gas, solar, wind etc.. The energy trilemma is about addressing three often conflicting challenges related to providing energy: ensuring energy security, providing energy accessibility, and achieving environmental sustainability.
### Sustainability
Environmental Sustainability of energy systems represents the transition of a country’s energy system towards mitigating and avoiding potential environmental harm and climate change impacts. The dimension focuses on productivity and efficiency of generation, transmission and distribution, decarbonisation, and air quality.
Globally, we draw most of our energy from oil, coal, and natural gas. These fossil fuels account for 80% of the world’s energy mix. These sources of energy have negative effects on our planet by releasing greenhouse gases into the atmosphere and are a huge contributor to the climate crisis. Sustainable energy focuses on meeting the energy demands of today without negatively impacting future generations. Hydro, solar, and wind power are all considered more sustainable sources of energy as they come from renewable sources. Other low-carbon options, such as nuclear power, may be a big part of our energy mix in the future but there are still ongoing debates about its sustainability when it comes to nuclear waste.
### Security
Security refers to whether we are able to access enough energy when and where we need it. This means being able to have uninterrupted availability of energy. In the short term this could mean an energy system that is able to respond to sudden changes in supply and demand. For example, energy demand in the UK spikes around 7am and again, between 4 and 7pm which is usually when people get up in the morning and when they return home from school or work!
Another aspect of energy security is security in the long term. With fossil fuels like oil, gas and coal, there is a limited supply and eventually these sources of energy will run out. Using renewable energy sources like wind, solar and hydro power can improve energy security in the long term.
### Accessibility
Accessibility relates to a country’s ability to provide universal access to reliable, affordable, and abundant energy for domestic and commercial use. The dimension captures basic access to electricity and clean cooking fuels and technologies, access to prosperity-enabling levels of energy consumption, and affordability of electricity, gas, and fuel.
We need energy to live our every day lives: to heat our homes, run our cars and public transport and power the lights in buildings. It is important that the energy that we use is affordable and accessible to everyone. According to the International Energy Agency’s 2020 report, solar power is the cheapest source of electricity in history, with wind power not too far behind. This is partly down to more efficient solar plants and wind turbines to allow for better energy generation.  However, issues include whether they allow reliable energy provision.
We can also improve energy affordability by making more energy efficient products. Gadgets that take less energy to power can help drive down energy costs by lessening demand.
Finally, please watch [this 3 minute video on the energy trilemma](https://www.youtube.com/watch?v=CI4DnLsANJM):
https://www.youtube.com/watch?v=CI4DnLsANJM
\clearpage
\newpage
# Load libraries
```{r warning=F,message=F}
library(quanteda)
library(quanteda.textstats)
library(quanteda.textplots)
library(stringr)
library(openxlsx)
library(ggplot2)
library(lme4)
library(MuMIn)
library(sjPlot)
library(irr)
library(DescTools)
library(lattice)
library(party)
```
# Load data
```{r eval=F,echo=F}
# Old code for loading text
copUK = NULL
engFiles = list.files("../data/HumanJudgements/stimuli/","*.txt")
for(file in engFiles){
tx = readLines(paste0("../data/HumanJudgements/stimuli/",file))
tx = paste(tx,collapse="\n")
tx = gsub("\n  [0-9][0-9]?\n","\n\n",tx)
tx = strsplit(tx,"\n [0-9][0-9]? : COP")[[1]]
idx = str_extract(tx,"[0-9][0-9]?_UK[0-9]+")
copID = sapply(idx,function(X){strsplit(X,"_")[[1]][1]})
copUK = rbind(copUK,
data.frame(
COP = paste0("COP",copID),
country = "UK",
ID = paste0("COP",idx),
title = "",
date = "",
source = "",
text = tx))
}
```
Load human ratings of articles:
```{r}
d4 = NULL
for(file in list.files("../data/HumanJudgements/judgements/")){
dx = NULL
if(grepl("xlsx",file)){
dx = read.xlsx(paste0("../data/HumanJudgements/judgements/",file),1)
}
if(grepl("csv",file)){
dx = read.csv(paste0("../data/HumanJudgements/judgements/",file),stringsAsFactors = F)
}
dx$participant = as.numeric(gsub("_","",substr(file,14,15)))
d4 = rbind(d4,dx)
}
# Exclude articles rated twice
d4 = d4[!duplicated(paste(d4$participant,d4$ID)),]
#d4$text = copUK[match(d4$ID,copUK$ID),]$text
d4$totalJudgement = d4$Sustainability + d4$Security + d4$Accessibility
getProp = function(dx,measure){
X = dx[,measure] / dx$totalJudgement
X[dx$totalJudgement==0] = 0
return(X)
}
d4$SustainabilityProp = getProp(d4,"Sustainability")
d4$SecurityProp = getProp(d4,"Security")
d4$AccessibilityProp = getProp(d4,"Accessibility")
d4$ID2 = paste0(d4$ID,"_",d4$participant)
d4$participant = factor(d4$participant)
```
Load keywords, but remove any that were suggested as a result of the analysis of the human judgements, to avoid circularity.
```{r}
kw = read.csv("../data/LEXIS/TrilemmaKeywords.csv",stringsAsFactors = F)
kw = kw[kw$Notes != "Suggested by human judgements",]
getKeywords= function(sub){
kx = unique(unlist(strsplit(kw[kw$Subject==sub,]$concepts,";")))
names(kx) = kx
return(kx)
}
accessibilityKeywords = getKeywords("Accessibility")
securityKeywords = getKeywords("Security")
sustainabilityKeywords = getKeywords("Sustainability")
```
Load reference corpus frequencies for alternative measure. The frequencies come from the SiBol Extended corpus of UK newspaper articles from the last 10 years (see Dunning, 1993; Partington, 2010), as made available on Sketch Engine (Kilgariff et al., 2014).
```{r}
refFreqAcc = read.csv("../data/EngBroadsheetNewspaperCorpus/acc.csv",
stringsAsFactors = F,skip=2)
refFreqSec = read.csv("../data/EngBroadsheetNewspaperCorpus/sec.csv",
stringsAsFactors = F,skip=2)
refFreqSus = read.csv("../data/EngBroadsheetNewspaperCorpus/sus.csv",
stringsAsFactors = F,skip=2)
```
Function to compare frequencies between two corpora, based on the G2 metric (see Rayson et al., 2004).
```{r}
logLikelihood.G2 = function(a,b,c,d){
# freqInCorpus1,freqInCorpus2,sizeOfCorpus1,sizeOfCorpus2
E1 = c*(a+b) / (c+d)
E2 = d*(a+b) / (c+d)
G2 = 2*((a*log(a/E1)) + (b*log(b/E2)))
G2[a==0] = NA
return(G2)
}
```
Function to load article text and calculate frequency scores:
```{r}
getTextFromParticipantFile = function(partNum,ids){
print(partNum)
fn = paste0("../data/HumanJudgements/stimuli/NewsArticles_",
partNum,"_SENT.txt")
tx = readLines(fn)
tx = paste(tx,collapse="\n")
tx = gsub("\n  [0-9][0-9]?\n","\n\n",tx)
tx = strsplit(tx,"\n [0-9][0-9]? : COP")[[1]]
idx = str_extract(tx,"[0-9][0-9]?_UK[0-9]+")
idx = paste0("COP",idx)
tx[match(ids,idx)]
}
processFile = function(d, accessibilityKW,securityKW,sustainabilityKW,
refFreqAcc,refFreqSec,refFreqSus){
# Get text from file sent to participant
d$text = ""
for(px in unique(d$participant)){
d[d$participant==px,]$text =
getTextFromParticipantFile(px,d[d$participant==px,]$ID)
}
# Lower case
d$text = tolower(d$text)
# some texts need to be borrowed from other files
d[is.na(d$text),]$text = d[match(d[is.na(d$text),]$ID, d$ID),]$text
# Create corpus, tokens, freq matrix
corp = corpus(d, docid_field = "ID2",text_field = "text")
tok = tokens(corp, remove_punct = TRUE)
corpDFM = dfm(tok)
d$ArticleTotalWords = rowSums(corpDFM)
# Get frequency for one keyword
getFrequency = function(keyword){
keyword = tolower(keyword)
if(grepl(" ",keyword)){
# Multi-word expression
return(sapply(str_extract_all(d$text, keyword),length))
}
if(keyword %in% colnames(corpDFM)){
return(as.vector(corpDFM[,keyword]))
}
return(rep(0,nrow(d)))
}
# Get score (frequency per 1000 words)
getScore = function(keywords){
freq = sapply(keywords,getFrequency)
return(rowSums(freq))
#prop = 1000000 * (freq/totalWords)
#return(prop)
}
d$CorpAccFreq = getScore(accessibilityKW)
d$CorpSecFreq = getScore(securityKW)
d$CorpSusFreq = getScore(sustainabilityKW)
d$CorpAccFreqRel = (1000000 * d$CorpAccFreq) / d$ArticleTotalWords
d$CorpSecFreqRel = (1000000 * d$CorpSecFreq) / d$ArticleTotalWords
d$CorpSusFreqRel = (1000000 * d$CorpSusFreq) / d$ArticleTotalWords
getScoreG2 = function(keywords,kFreq){
freq = sapply(keywords,getFrequency)
refFreq = kFreq[match(colnames(freq),kFreq$Item),]$Frequency
refFreq[is.na(refFreq)] = 0
G2s = sapply(1:nrow(freq),function(i){
logLikelihood.G2(freq[i,],refFreq,
d$ArticleTotalWords[i],482360)
})
return(colMeans(G2s,na.rm = T))
}
d$CorpSusFreqG2 = getScoreG2(sustainabilityKW,refFreqSus)
return(d)
}
```
```{r}
d4 = processFile(d4,
accessibilityKeywords, securityKeywords, sustainabilityKeywords,
refFreqAcc,refFreqSec,refFreqSus)
d4$Sustainability.scaled = d4$Sustainability/10
d4$CorpSusFreq.scaled = d4$CorpSusFreq/max(d4$CorpSusFreq)
```
\clearpage
\newpage
# Results
## Agreement between human participants
Estimate inter-rater reliability using Intraclass Correlation Coefficient:
```{r}
commonIDs = table(d4$ID)
commonIDs = names(commonIDs)[commonIDs>6]
irrx = d4[d4$ID %in% commonIDs,]
#irrx = irrx[!duplicated(paste(irrx$participant,irrx$ID)),]
irrx = irrx[order(irrx$participant,irrx$ID),]
irrx = matrix(irrx$Sustainability,
ncol=length(unique(irrx$participant)))
icc(irrx, model = "oneway")
parts = as.numeric(sort(unique(d4$participant)))
parts = parts[!parts %in% c(11)]
cors = matrix(NA,nrow=length(parts),ncol=length(parts))
rownames(cors) = parts
colnames(cors) = parts
d4 = d4[order(d4$ID),]
for(i in 1:length(parts)){
for(j in 1:length(parts)){
part1 = parts[i]
part2 = parts[j]
p1 = d4[d4$participant==part1 & d4$ID %in% commonIDs,]
p2 = d4[d4$participant==part2 & d4$ID %in% commonIDs,]
cors[i,j] = cor(p1$Sustainability,p2$Sustainability,
method = "kendall")
}
}
diag(cors)=NA
heatmap(cors,Rowv=NA,Colv="Rowv")
parts = as.numeric(sort(unique(d4$participant)))
cors = matrix(NA,nrow=length(parts),ncol=length(parts))
rownames(cors) = parts
colnames(cors) = parts
d4 = d4[order(d4$ID),]
for(i in 1:length(parts)){
for(j in 1:length(parts)){
part1 = parts[i]
part2 = parts[j]
p1 = d4[d4$participant==part1 & d4$ID %in% commonIDs,]
p2 = d4[d4$participant==part2 & d4$ID %in% commonIDs,]
cors[i,j] = cor(p1$Sustainability,p2$Sustainability,
method = "kendall")
}
}
diag(cors)=NA
heatmap(cors,Rowv=NA,Colv="Rowv")
as.numeric(sort(unique(d4$participant)))
table(d4$ID)
sort(table(d4$ID))
i = 1
j = 2
part1 = parts[i]
part2 = parts[j]
part1
part2
p1 = d4[d4$participant==part1 & d4$ID %in% commonIDs,]
p2 = d4[d4$participant==part2 & d4$ID %in% commonIDs,]
p1$Sustainability
p2$Sustainability
p1$ID
p2$ID
parts = as.numeric(sort(unique(d4$participant)))
cors = matrix(NA,nrow=length(parts),ncol=length(parts))
rownames(cors) = parts
colnames(cors) = parts
d4 = d4[order(d4$ID),]
for(i in 1:length(parts)){
for(j in 1:length(parts)){
part1 = parts[i]
part2 = parts[j]
print(c(part1,part2))
p1 = d4[d4$participant==part1 & d4$ID %in% commonIDs,]
p2 = d4[d4$participant==part2 & d4$ID %in% commonIDs,]
cors[i,j] = cor(p1$Sustainability,p2$Sustainability,
method = "kendall")
}
}
cors
diag(cors)=NA
heatmap(cors)
image(cors)
heatmap(cors,Rowv=NA)
heatmap(cors,Rowv=NA,Colv=NA)
mean(cors[c(-1,-8),c(-1,-8)],na.rm=T)
mean(cors[-1,-1],na.rm=T)
cors
cors[8,]
heatmap(cors,Rowv=NA,Colv=NA,symm = T)
heatmap(cors,Rowv=NA,Colv=NA,symm = T,revC = T)
colMeans(cors)
colMeans(cors,na.rm = T)
hist(cors[lower.tri(cors)])
